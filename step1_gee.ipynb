{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geojson\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely import geometry, ops\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "import geemap\n",
    "import ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskWater(image):\n",
    "    return image.updateMask(waterMask.select('water_mask').lt(1));\n",
    "\n",
    "# To do: combine the relief functions \n",
    "\n",
    "def computeRelief(feature):\n",
    "    max = ee.Number(feature.get('elev_max'));\n",
    "    min = ee.Number(feature.get('elev_min'));\n",
    "    \n",
    "    return feature.set('relief', max.subtract(min));\n",
    "\n",
    "def computeArcticRelief(feature):\n",
    "    emax = ee.Number(feature.get('aelev_max'));\n",
    "    emin = ee.Number(feature.get('aelev_min'));\n",
    "    \n",
    "    return feature.set('relief', emax.subtract(emin));\n",
    "\n",
    "def getCentroid(poly):\n",
    "    centroid = poly.geometry().centroid().coordinates()\n",
    "    return ee.Feature(poly).set('centroid', centroid)\n",
    "\n",
    "# MODIS water mask\n",
    "waterMask = (\n",
    "    ee.ImageCollection('MODIS/006/MOD44W')\n",
    "    .filter(ee.Filter.date('2015-01-01', '2015-01-02'))\n",
    "    .select('water_mask')\n",
    "    .first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment is an imagecollection so we mosaic to make an image\n",
    "segment = ee.ImageCollection(\"projects/sat-io/open-datasets/HYDROGRAPHY90/base-network-layers/segment\")\n",
    "\n",
    "flow_accumulation = ee.ImageCollection(\"projects/sat-io/open-datasets/HYDROGRAPHY90/base-network-layers/flow_accumulation\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get relevant climate and elevation variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get climate variables\n",
    "bio = ee.Image('WORLDCLIM/V1/BIO');\n",
    "MeanAP = bio.select('bio12');\n",
    "MeanAT = bio.select('bio01').multiply(0.1); #The variables are scaled by a factor of 10\n",
    "T_range = bio.select('bio07').multiply(0.1);\n",
    "\n",
    "# Get elevation (midlats use SRTM, high lats use ArcticDEM)\n",
    "elevation = ee.Image(\"USGS/SRTMGL1_003\").select('elevation').rename('elev');\n",
    "\n",
    "arctic =  ee.Image(\"UMN/PGC/ArcticDEM/V3/2m_mosaic\").select('elevation').rename('aelev')\n",
    "# for some reason you don't need to reproject in Python API....ok\n",
    "# arctic_stack = arctic.reproject('EPSG:4326').addBands(MeanAP).addBands(ndvi_raw).addBands(MeanAT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask river and NDVI data with water mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask watery NDVI\n",
    "ndvi = ee.ImageCollection('MODIS/006/MOD13Q1').select('NDVI')\n",
    "ndviMasked = ndvi.map(maskWater);\n",
    "ndvi_raw = ndviMasked.filter(ee.Filter.date('2020-01-01', '2021-01-01')).reduce(ee.Reducer.max())\n",
    "ndvi_raw = ndvi_raw.rename('ndvi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask watery rivers\n",
    "segment = segment.map(maskWater).mosaic().rename('segment') #imagecollection to image\n",
    "flow_accumulation = flow_accumulation.map(maskWater).mosaic().rename('flow_acc')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build data stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = (\n",
    "    elevation\n",
    "    .addBands(MeanAP)\n",
    "    .addBands(MeanAT)\n",
    "    .addBands(T_range)\n",
    "    .addBands(ndvi_raw)\n",
    "    .addBands(segment)\n",
    "    .addBands(flow_accumulation)\n",
    ")\n",
    "\n",
    "arctic_stack = (\n",
    "    arctic\n",
    "    .addBands(MeanAP)\n",
    "    .addBands(MeanAT)\n",
    "    .addBands(T_range)\n",
    "    .addBands(ndvi_raw)\n",
    "    .addBands(segment)\n",
    "    .addBands(flow_accumulation)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect midlat data from EE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "midlat_points = pd.DataFrame()\n",
    "\n",
    "xMin_vals = np.arange(-180.0, 190.0, 1)\n",
    "# xMin_vals = [0.0]\n",
    "\n",
    "for i, xMin in enumerate(xMin_vals): \n",
    "    e = datetime.datetime.now()\n",
    "    print (\"Time= %s:%s:%s\" % (e.hour, e.minute, e.second), 'xMin=',xMin)\n",
    "    try:\n",
    "        ## Extract data\n",
    "        # Load watersheds from a data table.\n",
    "        midlat_sheds = ee.FeatureCollection(\"WWF/HydroSHEDS/v1/Basins/hybas_10\")\\\n",
    "        .filterBounds(ee.Geometry.Rectangle([xMin, 23.4, xMin+1.0, 60.0]))\\\n",
    "        .filter(ee.Filter.equals(leftField = 'SUB_AREA', rightField = 'UP_AREA'))\n",
    "\n",
    "\n",
    "\n",
    "        midlat_reduced = stack.reduceRegions(\n",
    "          collection =  midlat_sheds,\n",
    "          reducer = (ee.Reducer.mean().combine(\n",
    "                  reducer2 = ee.Reducer.minMax(),\n",
    "                  sharedInputs=True)),\n",
    "                  scale = 1000,\n",
    "        );          \n",
    "\n",
    "        midlat_computed = midlat_reduced.filter(ee.Filter.notNull(ee.List(['elev_max']))).map(computeRelief).map(getCentroid)\n",
    "\n",
    "        midlat_threshold = midlat_computed.filter(ee.Filter.gt('ndvi_mean', 3900))\\\n",
    "        #.filter(ee.Filter.lt('bio12_mean', 800))\n",
    "        #.filter(ee.Filter.gt('relief', 150))\n",
    "\n",
    "        # Here I'll just count all the pixels in each layer of the stack instead of building two different stacks. Can always drop columns later\n",
    "        midlat_channel_counts = stack.reduceRegions(\n",
    "          reducer =  ee.Reducer.count(),\n",
    "          collection =  midlat_threshold\n",
    "          )\n",
    "        \n",
    "      \n",
    "\n",
    "        new = geemap.ee_to_pandas(midlat_channel_counts)\n",
    "\n",
    "        print('Number of sheds: ', len(new.index))\n",
    "        midlat_points = pd.concat([midlat_points,new])\n",
    "        midlat_points = midlat_points.reset_index(drop=True)\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted')\n",
    "        break\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midlat_points.to_csv(\"midlat_points.csv\")\n",
    "\n",
    "midlat_points.loc[:, 'long'] = midlat_points.centroid.map(lambda x: x[0])\n",
    "midlat_points.loc[:, 'lat'] = midlat_points.centroid.map(lambda x: x[1])\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    midlat_points, geometry=gpd.points_from_xy(midlat_points.long, midlat_points.lat))\n",
    "\n",
    "gdf.drop('centroid', axis=1).to_file(\"midlat_h90.shp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Arctic data from EE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_points = pd.DataFrame()\n",
    "\n",
    "xMin_vals = np.arange(-180.0, 190.0, 1)\n",
    "# xMin_vals = [-152]\n",
    "\n",
    "for i, xMin in enumerate(xMin_vals): \n",
    "    e = datetime.datetime.now()\n",
    "    try:\n",
    "      print (\"Time= %s:%s:%s\" % (e.hour, e.minute, e.second), 'xMin=',xMin)\n",
    "\n",
    "      ## Extract data\n",
    "      # Load watersheds from a data table.\n",
    "      sheds_a = ee.FeatureCollection(\"WWF/HydroSHEDS/v1/Basins/hybas_10\")\\\n",
    "      .filterBounds(ee.Geometry.Rectangle([xMin, 60.0, xMin+1.0, 90.0]))\\\n",
    "      .filter(ee.Filter.equals(leftField = 'SUB_AREA', rightField = 'UP_AREA'))\n",
    "\n",
    "\n",
    "      arctic_reduced = arctic_stack.reduceRegions(\n",
    "        collection =  sheds_a,\n",
    "        reducer = (ee.Reducer.mean().combine(\n",
    "                reducer2 = ee.Reducer.minMax(),\n",
    "                sharedInputs=True)),\n",
    "                scale = 1000,\n",
    "      );          \n",
    "\n",
    "      arctic_computed = arctic_reduced.filter(ee.Filter.notNull(ee.List(['aelev_max']))).map(computeArcticRelief).map(getCentroid)\n",
    "\n",
    "      arctic_threshold = arctic_computed.filter(ee.Filter.gt('ndvi_mean', 3900))\\\n",
    "      #.filter(ee.Filter.lt('bio12_mean', 800))\n",
    "      #.filter(ee.Filter.gt('relief', 150))\n",
    "\n",
    "      # Here I'll just count all the pixels in each layer of the stack instead of building two different stacks. Can always drop columns later\n",
    "      arctic_channel_counts = arctic_stack.select(['segment', 'flow_acc']).reduceRegions(\n",
    "        reducer =  ee.Reducer.count(),\n",
    "        scale = 30,\n",
    "        collection =  arctic_threshold\n",
    "        )\n",
    "      new = geemap.ee_to_pandas(arctic_channel_counts)\n",
    "      print('Number of sheds: ', len(new.index))\n",
    "      arctic_points = pd.concat([arctic_points,new])\n",
    "      arctic_points = arctic_points.reset_index()\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted')\n",
    "        break\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arctic_points.to_csv(\"arctic_points_new.csv\")\n",
    "\n",
    "arctic_points = arctic_points.drop('level_0', axis=1).reset_index()\n",
    "\n",
    "arctic_points.loc[:, 'long'] = arctic_points.centroid.map(lambda x: x[0])\n",
    "arctic_points.loc[:, 'lat'] = arctic_points.centroid.map(lambda x: x[1])\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    arctic_points, geometry=gpd.points_from_xy(arctic_points.long, arctic_points.lat))\n",
    "\n",
    "gdf.drop('centroid', axis=1).to_file(\"arctic_h90.shp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
