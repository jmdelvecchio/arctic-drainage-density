{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../lsdviztools') # remove when updated on main \n",
    "import glob\n",
    "import lsdviztools.lsdmapwrappers as lsdmw\n",
    "from lsdviztools.lsdplottingtools import lsdmap_gdalio as gio\n",
    "from IPython.utils import io\n",
    "\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.channels import LocalChannel\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.providers import LocalProvider\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from osgeo import gdal_array\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams.update({\"pdf.fonttype\":42})\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you load in the results after you ran through once you don't have to do it again (I think?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shed_list = pd.read_csv(\"shed_list_for_dd_new.csv\", dtype = {'HYBAS_ID':'int', 'DD':'float64'},\n",
    "                        #  index_col='HYBAS_ID'\n",
    "                         )\n",
    "from pathlib import Path\n",
    "p = Path('./lsdtt/')\n",
    "id_list = [f.name for f in p.iterdir() if f.is_dir()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LSDTT algorithms on your new DEMs, extract and save data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do  lsdtt channel extraction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htex_local = Config(\n",
    "    executors = [\n",
    "        HighThroughputExecutor(\n",
    "            max_workers = 15, # Caps the number of workers launched per node\n",
    "            provider = LocalProvider(\n",
    "                worker_init=\"module load conda; conda activate lsdtopy\",\n",
    "                max_blocks = 1\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "parsl.clear()\n",
    "parsl.load(htex_local)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_app\n",
    "def run_lsdtt(id):\n",
    "    import sys\n",
    "    sys.path.insert(0, '../lsdviztools') # remove when updated on main \n",
    "    from pathlib import Path\n",
    "    import glob\n",
    "    import lsdviztools.lsdmapwrappers as lsdmw\n",
    "    from lsdviztools.lsdplottingtools import lsdmap_gdalio as gio\n",
    "    from IPython.utils import io\n",
    "\n",
    "    surface_fitting_radius = '100'\n",
    "    m_over_n = '0.5'    \n",
    "\n",
    "    p = Path(f'./lsdtt/{id}/')\n",
    "    \n",
    "    if (p / f'{id}_UTM_mn_0.5_D_CN.csv').exists() == True:\n",
    "        return f'{id} already processed'\n",
    "\n",
    "    else:\n",
    "        print(f'Now working on {id}')\n",
    "        DataDirectory = f'./lsdtt/{id}/'\n",
    "        RasterFile = f'{id}.tif'\n",
    "    try:\n",
    "        gio.convert4lsdtt(DataDirectory, RasterFile,minimum_elevation=0.1,resolution=10)\n",
    "\n",
    "        lsdtt_parameters = {\n",
    "                            # Table 3\n",
    "                            \"carve_before_fill\" : \"true\", # in case there are dams, \n",
    "                            \"raster_is_filled\" : \"false\",\n",
    "                            # Table 4\n",
    "                            \"print_area_threshold_channels\" : \"true\",\n",
    "                            \"print_dreich_channels\" : \"true\",\n",
    "                            # Table 5\n",
    "                            # \"write_hillshade\" : \"true\",\n",
    "                            \"print_channels_to_csv\" : \"true\",\n",
    "                            # \"print_curvature_raster\" : \"true\", # doesn't work\n",
    "                            \"print_tangential_curvature\": \"true\",\n",
    "                            # Table 6\n",
    "                            \"surface_fitting_radius\" : surface_fitting_radius, \n",
    "                            \"threshold_contributing_pixels\" : '5000', # Equivalent to threshold from MERIT for 10m DEM\n",
    "                            \"A_0\" : '1.0', # Default 1.0, used by DrEICH\n",
    "                            #### Change the m/n!\n",
    "                            # \"m_over_n\" : 0.5, # Default 0.5, used by DrEICH\n",
    "                            \"m_over_n\" : m_over_n, # Default 0.5, used by DrEICH\n",
    "                            #####\n",
    "                            # Table 7\n",
    "                            \"print_dinf_drainage_area_raster\" : \"true\",\n",
    "                            # \"print_d8_drainage_area_raster\" : \"true\",\n",
    "                            \"pruning_drainage_area\" : \"10\",\n",
    "                            \"connected_components_threshold\" : \"10\"\n",
    "                            \n",
    "                            }\n",
    "        r_prefix = f'{id}_UTM'\n",
    "        w_prefix = f'{id}_{surface_fitting_radius}m_mn_{m_over_n}'\n",
    "        lsdtt_drive = lsdmw.lsdtt_driver(command_line_tool = \"lsdtt-channel-extraction\", \n",
    "                                        read_prefix = r_prefix,\n",
    "                                        write_prefix= w_prefix,\n",
    "                                        read_path = f'./lsdtt/{id}/',\n",
    "                                        write_path = f'./lsdtt/{id}/',\n",
    "                                        parameter_dictionary=lsdtt_parameters)\n",
    "        lsdtt_drive.print_parameters()\n",
    "        lsdtt_drive.run_lsdtt_command_line_tool()\n",
    "        \n",
    "        return id\n",
    "    except AttributeError: # Maybe some data are weird bc \"'NoneType' object has no attribute 'GetProjection'\"\n",
    "       return f'{id} is bad'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_app_futures = []\n",
    "\n",
    "for id in id_list:\n",
    "    app_future = run_lsdtt(id)\n",
    "    all_app_futures.append(app_future)\n",
    "\n",
    "# By getting the `result()` of each app future, this block won't continue to \n",
    "# the print statement until all the files are staged.\n",
    "[app_future.result() for app_future in all_app_futures] \n",
    "\n",
    "print(\"All IDs have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown and clear the parsl executor\n",
    "htex_local.executors[0].shutdown()\n",
    "parsl.clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze lsdtt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_fitting_radius = '100'\n",
    "m_over_n = '0.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('./lsdtt/')\n",
    "id_list = [f.name for f in p.iterdir() if f.is_dir()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate drainage density "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "no_bils = []\n",
    "no_data = []\n",
    "no_channels = []\n",
    "\n",
    "\n",
    "\n",
    "for id in id_list:\n",
    "    w_prefix = f'{id}_{surface_fitting_radius}m_mn_{m_over_n}'\n",
    "\n",
    "    AT_CN = f'{w_prefix}_AT_CN.csv'\n",
    "    D_CN = f'{w_prefix}_D_CN.csv'\n",
    "    dinf =  f'{w_prefix}_dinf_area.bil'\n",
    "    # tan_curv = f'{id}_UTM_{surface_fitting_radius}m_TANCURV.bil'\n",
    "\n",
    "    # d8 =  f'{w_prefix}_d8_area.bil'\n",
    "\n",
    "    filepath = (p / f'{id}' / f'{dinf}')\n",
    "\n",
    "    try: \n",
    "        # Read raster data as numeric array from file\n",
    "        rasterArray = gdal_array.LoadFile(filepath.as_posix())\n",
    "        #Create a masked array for making calculations without nodata values\n",
    "        rasterArray = np.ma.masked_equal(rasterArray, -9999.).compressed()\n",
    "\n",
    "        rastersize = len(rasterArray)\n",
    "\n",
    "        if rastersize != 0:\n",
    "            with open((p / f'{id}' / f'{AT_CN}').as_posix(),\"r\") as f:\n",
    "                reader = csv.reader(f,delimiter = \",\")\n",
    "                data = list(reader)\n",
    "                AT_CN_count = len(data) - 1 \n",
    "\n",
    "            try:    \n",
    "                with open((p / f'{id}' / f'{D_CN}').as_posix(),\"r\") as f:\n",
    "                    reader = csv.reader(f,delimiter = \",\")\n",
    "                    data = list(reader)\n",
    "                    D_CN_count = len(data) - 1\n",
    "            except FileNotFoundError:\n",
    "                print(f'No DrEICH channels in {id}')\n",
    "                D_CN_count = np.nan\n",
    "                no_channels.append([id])\n",
    "\n",
    "            data = [id, (AT_CN_count/rastersize), (D_CN_count/rastersize)]\n",
    "        else:\n",
    "            print(f'Raster of size 0 for {id}')\n",
    "            no_data.append([id])\n",
    "            data = [id,np.nan,np.nan]\n",
    "\n",
    "    except ValueError:\n",
    "        print(f'No .bil made for for {id}')\n",
    "        data = [id,np.nan,np.nan]\n",
    "        no_bils.append([id])\n",
    "    \n",
    "    data_list.append(data)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=data_list, columns=['HYBAS_ID', 'AT_CN_DD', 'D_CN_DD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['HYBAS_ID']\n",
    "\n",
    "with open('no_channels.csv', 'w') as f:\n",
    "  # using csv.writer method from CSV package\n",
    "  write = csv.writer(f)\n",
    "    \n",
    "  write.writerow(fields)\n",
    "  write.writerows(no_channels)\n",
    "\n",
    "with open('no_bils.csv', 'w') as f:\n",
    "  # using csv.writer method from CSV package\n",
    "  write = csv.writer(f)\n",
    "    \n",
    "  write.writerow(fields)\n",
    "  write.writerows(no_bils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shed_list['HYBAS_ID'] = shed_list['HYBAS_ID'].astype(str)\n",
    "\n",
    "merged = shed_list.merge(df.dropna(), on=\"HYBAS_ID\")\n",
    "\n",
    "merged.to_csv(\"lsdtt_results.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.read_csv(\"lsdtt_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = merged[merged['EXTENT'] != 'No permafrost']\n",
    "noperm = merged[merged['EXTENT'] == 'No permafrost']\n",
    "continuous = merged[merged['EXTENT']=='Continuous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "im1 = ax.hexbin(merged['DD'],merged['D_CN_DD'], gridsize=(70,50), mincnt=1, cmap='plasma')\n",
    "im0 = sns.regplot(x='DD', y='D_CN_DD',  fit_reg=True,\n",
    "                  scatter_kws={'alpha':0.0,\n",
    "                              's':5},\n",
    "                  ax=ax,\n",
    "                  data=noperm,\n",
    "                  line_kws={'color':'k'},\n",
    "                 color='k')\n",
    "cb = fig.colorbar(im1)\n",
    "cb.set_label(\"Watershed counts in hex bin\")\n",
    "ax.set_xlim(0.09,0.20)\n",
    "ax.set_ylim(0,0.075)\n",
    "ax.set_xlabel(\"Hydrography90m channel drainage density (pix/pix)\")\n",
    "ax.set_ylabel(\"DrEICH algorithm channel drainage density (pix/pix)\")\n",
    "plt.savefig('./figure_outputs/supp_DD_method_comparison.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "r, p = sp.stats.pearsonr(merged['DD'], merged['D_CN_DD'])\n",
    "\n",
    "print(r,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "re = sm.OLS(merged['D_CN_DD'], merged['DD']).fit()\n",
    "print(re.summary())\n",
    "merged['residual_DD'] = re.resid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.groupby(['EXTENT'])['residual_DD'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flow accumulation distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = {}\n",
    "\n",
    "p = Path(f'./lsdtt/')\n",
    "\n",
    "surface_fitting_radius = '100'\n",
    "m_over_n = '0.5'  \n",
    "\n",
    "for id in id_list:\n",
    "    w_prefix = f'{id}_{surface_fitting_radius}m_mn_{m_over_n}'\n",
    "\n",
    "    dinf =  f'{w_prefix}_dinf_area.bil'\n",
    "\n",
    "    p = Path(f'./lsdtt/')\n",
    "\n",
    "    filepath = (p / f'{id}' / f'{dinf}')\n",
    "\n",
    "    try: \n",
    "        # Read raster data as numeric array from file\n",
    "        rasterArray = gdal_array.LoadFile(filepath.as_posix())\n",
    "        #Create a masked array for making calculations without nodata values\n",
    "        rasterArray = np.ma.masked_equal(rasterArray, -9999.).compressed()\n",
    "\n",
    "        hist, edges = np.histogram(rasterArray, np.logspace(2,7,70))\n",
    "\n",
    "        hist_dict[id] = hist\n",
    "\n",
    "    except ValueError:\n",
    "        print(f'No .bil made for for {id}')\n",
    "        # data = [id,np.nan,np.nan]\n",
    "        # no_bils.append([id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_list = []\n",
    "noperm_list = []\n",
    "c_list = []\n",
    "\n",
    "for id in noperm['HYBAS_ID'].astype(str):\n",
    "    if id in hist_dict:\n",
    "        arr = hist_dict[id]/hist_dict[id].sum()\n",
    "        # exceedence = np.cumsum(arr[::-1])[::-1]\n",
    "        # noperm_list.append(exceedence)\n",
    "        cad = np.cumsum(arr)\n",
    "        noperm_list.append(cad)\n",
    "\n",
    "for id in perm['HYBAS_ID'].astype(str):\n",
    "    if id in hist_dict:\n",
    "        arr = hist_dict[id]/hist_dict[id].sum()\n",
    "        # exceedence = np.cumsum(arr[::-1])[::-1]\n",
    "        # perm_list.append(exceedence)\n",
    "        cad = np.cumsum(arr)\n",
    "        perm_list.append(cad)              \n",
    "                        \n",
    "\n",
    "for id in continuous['HYBAS_ID'].astype(str):\n",
    "    if id in hist_dict:\n",
    "        arr = hist_dict[id]/hist_dict[id].sum()\n",
    "        # exceedence = np.cumsum(arr[::-1])[::-1]\n",
    "        # c_list.append(exceedence)\n",
    "        cad = np.cumsum(arr)\n",
    "        c_list.append(cad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic = gpd.read_file('../arctic_h90.shp').drop_duplicates(subset='HYBAS_ID', keep=\"first\")\n",
    "midlat = gpd.read_file('../midlat_h90.shp').drop_duplicates(subset='HYBAS_ID', keep=\"first\")\n",
    "joined = pd.concat([arctic, midlat]).drop_duplicates(subset='HYBAS_ID', keep=\"first\")\n",
    "joined = joined.set_crs(\"EPSG:4326\").to_crs('EPSG:5936')\n",
    "# Dumb, move this\n",
    "joined['DD'] = joined['segment']/joined['flow_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined['HYBAS_ID'] = joined['HYBAS_ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['HYBAS_ID'] = merged['HYBAS_ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts = merged.merge(joined, on=\"HYBAS_ID\")\n",
    "\n",
    "permafrost = atts[atts['EXTENT'] == 'Continuous']\n",
    "nonpermafrost = atts[atts['EXTENT'] == 'No permafrost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts=5\n",
    "map_max=700\n",
    "relief_max=1200\n",
    "map_array=np.append(np.linspace(-1,map_max,num=pts), [np.inf])\n",
    "relief_array=np.append(np.linspace(-1,relief_max,num=pts), [np.inf])\n",
    "\n",
    "nonpermafrost['bin_map'] = pd.cut(nonpermafrost['bio12_mean'],bins=map_array, labels=np.arange(len(map_array)-1))\n",
    "nonpermafrost['bin_relief'] = pd.cut(nonpermafrost['relief'],bins=relief_array, labels=np.arange(len(relief_array)-1))\n",
    "nonpermafrost_out = nonpermafrost.groupby(['bin_map','bin_relief'])['D_CN_DD']\n",
    "\n",
    "nonpermafrost_counts = nonpermafrost_out.count().sort_index().values.reshape(len(relief_array)-1,len(map_array)-1)\n",
    "\n",
    "nonpermafrost_arr = nonpermafrost_out.mean().sort_index().values.reshape(len(relief_array)-1,len(map_array)-1)\n",
    "\n",
    "permafrost['bin_map'] = pd.cut(permafrost['bio12_mean'],bins=map_array, labels=np.arange(len(map_array)-1),\n",
    "                             )\n",
    "permafrost['bin_relief'] = pd.cut(permafrost['relief'],bins=relief_array, labels=np.arange(len(relief_array)-1),\n",
    "                              )\n",
    "permafrost_out = permafrost.groupby(['bin_map','bin_relief'])['D_CN_DD']\n",
    "\n",
    "permafrost_counts = permafrost_out.count().sort_index().values.reshape(len(relief_array)-1,len(map_array)-1)\n",
    "\n",
    "permafrost_arr = permafrost_out.mean().sort_index().values.reshape(len(relief_array)-1,len(map_array)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import itertools\n",
    "p = np.empty([pts,pts])\n",
    "p[:] = np.nan\n",
    "for i in itertools.product(range(pts),range(pts)):\n",
    "    try:\n",
    "        U1, p[i] = scipy.stats.mannwhitneyu(\n",
    "        permafrost_out.get_group(i),\n",
    "        nonpermafrost_out.get_group(i))\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shed_count_thresh = 0\n",
    "\n",
    "pm1 = np.ma.masked_less(p, 1.00e-04)\n",
    "pm1_masked = np.ma.masked_where(permafrost_counts < shed_count_thresh, pm1)\n",
    "pm1_masked = np.ma.masked_where(nonpermafrost_counts < shed_count_thresh, pm1_masked)\n",
    "\n",
    "\n",
    "pm2 = np.ma.masked_inside(p, 1.00e-04, 1.00e-03)\n",
    "pm2_masked = np.ma.masked_where(permafrost_counts < shed_count_thresh, pm2)\n",
    "pm2_masked = np.ma.masked_where(nonpermafrost_counts < shed_count_thresh, pm2_masked)\n",
    "\n",
    "ratio = permafrost_arr/nonpermafrost_arr\n",
    "ratio_masked = np.ma.masked_where(permafrost_counts < shed_count_thresh, ratio)\n",
    "ratio_masked = np.ma.masked_where(nonpermafrost_counts < shed_count_thresh, ratio_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = permafrost_counts+nonpermafrost_counts\n",
    "counts_masked = np.ma.masked_where(permafrost_counts == 0, counts)\n",
    "counts_masked = np.ma.masked_where(nonpermafrost_counts == 0, counts_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(counts_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "\n",
    "fig, ax = plt.subplots(3,1,figsize=(3,3),dpi=300, sharey=True, sharex=True)\n",
    "\n",
    "im0 = ax[0].pcolor(ratio, cmap='PRGn', \n",
    "               #extent=[np.min(relief_array), np.max(relief_array), np.min(map_array), np.max(map_array)],\n",
    "                vmin=0.5, vmax=1.5,\n",
    "                   )\n",
    "\n",
    "# im1 = ax[1].pcolor(permafrost_counts+nonpermafrost_counts)\n",
    "\n",
    "im2 = ax[1].pcolor(pm2_masked,\n",
    "                   norm=colors.LogNorm(vmin=1e-4,vmax=1e-1,),\n",
    "                   \n",
    "                  #   hatch='//',\n",
    "                  #   alpha=0\n",
    "                    )\n",
    "\n",
    "im3 = ax[2].pcolor(counts_masked,\n",
    "                  #  norm=colors.LogNorm()\n",
    "                  #   hatch='//',\n",
    "                  #   alpha=0\n",
    "                  cmap='magma'\n",
    "                    )\n",
    "\n",
    "\n",
    "cb0 = fig.colorbar(im0)\n",
    "cb0.set_label(\"Drainage density\\nratio (permafrost\\n/nonpermafrost)\", fontsize=6)\n",
    "cb2 = fig.colorbar(im2)\n",
    "cb2.set_label(\"p value,\\nMann-Whitney U\", fontsize=6)\n",
    "cb3 = fig.colorbar(im3)\n",
    "cb3.set_label(\"Watershed counts\\nin bin (total permafrost\\n+nonpermafrost)\", fontsize=6)\n",
    "\n",
    "ax[0].set_xticks([0,2.5,5])\n",
    "ax[0].set_xticklabels(['0',str(relief_max/2),'>'+str(relief_max)])\n",
    "\n",
    "ax[0].set_yticks([0,2.5,5])\n",
    "ax[0].set_yticklabels(['0',str(map_max/2),'>'+str(map_max)])\n",
    "\n",
    "fig.tight_layout()\n",
    "ax[2].set_xlabel(\"Relief (m)\")\n",
    "ax[0].set_ylabel(\"MAP (mm/yr)\")\n",
    "plt.savefig('./figure_outputs/supp_dreich_ratios.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now get curvature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is just a one time thing so I don't rerun everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htex_local = Config(\n",
    "    executors = [\n",
    "        HighThroughputExecutor(\n",
    "            max_workers = 10, # Caps the number of workers launched per node\n",
    "            provider = LocalProvider(\n",
    "                worker_init=\"module load conda; conda activate lsdtopy\",\n",
    "                max_blocks = 1\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "parsl.clear()\n",
    "parsl.load(htex_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_app\n",
    "def get_curve_and_jct_lsdtt(id):\n",
    "    import sys\n",
    "    sys.path.insert(0, '../lsdviztools') # remove when updated on main \n",
    "    from pathlib import Path\n",
    "    import glob\n",
    "    import lsdviztools.lsdmapwrappers as lsdmw\n",
    "    from lsdviztools.lsdplottingtools import lsdmap_gdalio as gio\n",
    "    from IPython.utils import io\n",
    "\n",
    "    surface_fitting_radius = '100'\n",
    "    m_over_n = '0.5'    \n",
    "\n",
    "    p = Path(f'./lsdtt/{id}/')\n",
    "    \n",
    "    print(f'Now working on {id}')\n",
    "\n",
    "    try:\n",
    "        # gio.convert4lsdtt(DataDirectory, RasterFile,minimum_elevation=0.1,resolution=10)\n",
    "\n",
    "        basic_parameters = {\n",
    "                                # Table 3\n",
    "                                \"carve_before_fill\" : \"true\", # in case there are dams, \n",
    "                                \"raster_is_filled\" : \"false\",\n",
    "                                # Table 4\n",
    "                                # Table 5\n",
    "                                # \"write_hillshade\" : \"true\",\n",
    "                                # \"print_channels_to_csv\" : \"true\",\n",
    "                                \"print_curvature\" : \"true\",\n",
    "                                \"print_tangential_curvature\": \"true\",\n",
    "                                # Table 6\n",
    "                                \"surface_fitting_radius\" : surface_fitting_radius, \n",
    "                                \"threshold_contributing_pixels\" : '500', # Equivalent to threshold from Hydrography90m for 10m DEM\n",
    "                                \"A_0\" : '1.0', # Default 1.0, used by DrEICH\n",
    "                                #### Change the m/n!\n",
    "                                # \"m_over_n\" : 0.5, # Default 0.5, used by DrEICH\n",
    "                                \"m_over_n\" : m_over_n, # Default 0.5, used by DrEICH\n",
    "                                #####\n",
    "\n",
    "                                ## more secrets\n",
    "\n",
    "                                # \"print_junction_angles_to_csv\" : \"true\",\n",
    "                                # \"print_junction_angles_to_csv_in_basins\": \"true\",\n",
    "                                # \"print_area_threshold_channels\" : \"true\",\n",
    "\n",
    "                                }\n",
    "\n",
    "\n",
    "        r_prefix = f'{id}_UTM'\n",
    "        w_prefix = f'{id}_{surface_fitting_radius}m_mn_{m_over_n}'\n",
    "\n",
    "        # basic metrics does junction angle and curvature for some reason \n",
    "\n",
    "        lsdtt_drive = lsdmw.lsdtt_driver(\n",
    "                                        # command_line_tool = \"lsdtt-channel-extraction\",\n",
    "                                        command_line_tool = \"lsdtt-basic-metrics\",\n",
    "                                        read_prefix = r_prefix,\n",
    "                                        write_prefix= f'{w_prefix}',\n",
    "                                        read_path = f'./lsdtt/{id}/',\n",
    "                                        write_path = f'./lsdtt/{id}/',\n",
    "                                        parameter_dictionary=basic_parameters)\n",
    "\n",
    "        lsdtt_drive.print_parameters()\n",
    "        lsdtt_drive.run_lsdtt_command_line_tool()\n",
    "\n",
    "        # lsdtt_drive = lsdmw.lsdtt_driver(\n",
    "        #                                 command_line_tool = \"lsdtt-channel-extraction\",\n",
    "        #                                 # command_line_tool = \"lsdtt-basic-metrics\",\n",
    "        #                                 read_prefix = r_prefix,\n",
    "        #                                 write_prefix= w_prefix,\n",
    "        #                                 read_path = f'./lsdtt/{id}/',\n",
    "        #                                 parameter_dictionary=channel_parameters)\n",
    "\n",
    "        # lsdtt_drive.print_parameters()\n",
    "        # lsdtt_drive.run_lsdtt_command_line_tool()\n",
    "        \n",
    "        return id\n",
    "    except AttributeError: # Maybe some data are weird bc \"'NoneType' object has no attribute 'GetProjection'\"\n",
    "       return f'{id} is bad'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_app_futures = []\n",
    "\n",
    "for id in id_list:\n",
    "    app_future = get_curve_and_jct_lsdtt(id)\n",
    "    all_app_futures.append(app_future)\n",
    "\n",
    "# By getting the `result()` of each app future, this block won't continue to \n",
    "# the print statement until all the files are staged.\n",
    "[app_future.result() for app_future in all_app_futures] \n",
    "\n",
    "print(\"All IDs have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown and clear the parsl executor\n",
    "htex_local.executors[0].shutdown()\n",
    "parsl.clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curvature area plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curve_area(id):\n",
    "    \n",
    "    from scipy.stats import binned_statistic\n",
    "\n",
    "    w_prefix = f'{id}_{surface_fitting_radius}m_mn_{m_over_n}'\n",
    "\n",
    "    dinf_file =  f'{w_prefix}_dinf_area.bil'\n",
    "\n",
    "    curvature_file =  f'{w_prefix}_500px_for_junction_angles_TANCURV.bil'\n",
    "\n",
    "    p = Path(f'./lsdtt/')\n",
    "\n",
    "    dinf_filepath = (p / f'{id}' / f'{dinf_file}')\n",
    "\n",
    "    curvature_filepath = (p / f'{id}' / f'{curvature_file}')\n",
    "\n",
    "    try: \n",
    "        # Read raster data as numeric array from file\n",
    "        dinf = gdal_array.LoadFile(dinf_filepath.as_posix())\n",
    "        #Create a masked array for making calculations \n",
    "        dinf = np.ma.masked_equal(dinf, -9999.).flatten()\n",
    "        # Read raster data as numeric array from file\n",
    "        curvature = gdal_array.LoadFile(curvature_filepath.as_posix())\n",
    "        #Create a masked array for making calculations \n",
    "        curvature = np.ma.masked_equal(curvature, -9999.).flatten()\n",
    "\n",
    "        bin_edges = np.logspace(2,7,70)\n",
    "\n",
    "        med_stat_ca = binned_statistic(dinf,curvature,\n",
    "                                statistic='median',\n",
    "                                bins=bin_edges)\n",
    "        \n",
    "        stats_dict[id] = med_stat_ca.statistic\n",
    "\n",
    "\n",
    "    except ValueError:\n",
    "        print(f'No .bil made for for {id}')\n",
    "        # data = [id,np.nan,np.nan]\n",
    "        # no_bils.append([id])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict = {}\n",
    "\n",
    "p = Path(f'./lsdtt/')\n",
    "\n",
    "id_list = [f.name for f in p.iterdir() if f.is_dir()]\n",
    "\n",
    "for id in id_list:\n",
    "    med_stat_ca = curve_area(id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look, this is criminal and i dont care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.read_csv(\"lsdtt_results.csv\")\n",
    "merged['HYBAS_ID'] = merged['HYBAS_ID'].astype(str)\n",
    "merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noperm = merged[merged['EXTENT'] == 'No permafrost']\n",
    "continuous = merged[merged['EXTENT']=='Continuous']\n",
    "\n",
    "perm_list_c = []\n",
    "noperm_list_c = []\n",
    "c_list_c = []\n",
    "\n",
    "for id in noperm['HYBAS_ID']:\n",
    "    if id in stats_dict:\n",
    "        arr = stats_dict[id]\n",
    "        noperm_list_c.append(arr)\n",
    "\n",
    "for id in continuous['HYBAS_ID']:\n",
    "    if id in stats_dict:\n",
    "        arr = stats_dict[id]\n",
    "        c_list_c.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noperm_list_c_scaled = np.nanmedian(noperm_list_c, axis=0) * 1e5\n",
    "c_list_c_scaled = np.nanmedian(c_list_c, axis=0) * 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "fig, ax = plt.subplots(2,1,\n",
    "                       sharex=True,\n",
    "                       sharey=True,\n",
    "                        figsize=(3.4252,3.4252),dpi=300,\n",
    "                        gridspec_kw={'height_ratios':[1,0.8]})\n",
    "# for i, arr in enumerate(perm_list):\n",
    "#     ax.plot(np.logspace(2,7,70)[:-1], perm_list[i], 'b', alpha=0.03)\n",
    "# for i,arr in enumerate(noperm_list):    \n",
    "#     ax.plot(np.logspace(2,7,70)[:-1], noperm_list[i], 'r', alpha=0.03)\n",
    "# ax.fill_between(np.logspace(2,7,70)[:-1],\n",
    "#                 np.mean(c_list, axis=0)-np.std(c_list, axis=0),\n",
    "#                 np.mean(c_list, axis=0)+np.std(c_list, axis=0),\n",
    "#                 color='gray', alpha=0.1)\n",
    "# ax.fill_between(np.logspace(2,7,70)[:-1],\n",
    "#                 np.mean(noperm_list, axis=0)-np.std(noperm_list, axis=0),\n",
    "#                 np.mean(noperm_list, axis=0)+np.std(noperm_list, axis=0),\n",
    "#                 color='darkgray', alpha=0.1)\n",
    "ax[0].plot(np.logspace(2,7,70)[:-1], np.median(c_list, axis=0), 'lightsteelblue', linewidth=2, zorder=1)\n",
    "ax[1].plot(np.logspace(2,7,70)[:-1], np.median(noperm_list, axis=0), 'lightcoral', linewidth=2, zorder=1)\n",
    "\n",
    "ax[1].scatter(np.logspace(2,7,70)[:-1], np.median(noperm_list, axis=0), s=20, c=noperm_list_c_scaled, zorder=2, vmin=-10, vmax=10, cmap='BrBG',\n",
    "            edgecolor='lightcoral',\n",
    "              linewidth=0.5,\n",
    "            )\n",
    "forthecolors = ax[0].scatter(np.logspace(2,7,70)[:-1], np.median(c_list, axis=0), s=20, c=c_list_c_scaled, zorder=2, vmin=-10, vmax=10, cmap='BrBG',\n",
    "                           edgecolor='lightsteelblue',\n",
    "                             linewidth=0.5,\n",
    "                           )\n",
    "\n",
    "fmt = matplotlib.ticker.ScalarFormatter(useMathText=True)\n",
    "cb0 = fig.colorbar(forthecolors, location=\"top\", format=fmt)\n",
    "cb0.set_label(\"Curvature (m$^-$$^1$) x 10$^-$$^5$\")\n",
    "\n",
    "\n",
    "ax[0].axvline(np.interp(0, np.nanmedian(c_list_c, axis=0), np.logspace(2,7,70)[:-1]), 0, 0.8, color = 'lightsteelblue', linestyle=\":\", zorder=0)\n",
    "ax[1].axvline(np.interp(0, np.nanmedian(noperm_list_c, axis=0), np.logspace(2,7,70)[:-1]), 0.70, 1, color='lightcoral', linestyle=':', zorder=0)\n",
    "\n",
    "\n",
    "ax[0].set_xlim(9e1, 1e5)\n",
    "ax[0].set_ylim(0, 1)\n",
    "ax[0].set_xscale(\"log\")\n",
    "\n",
    "ax[1].set_xlabel(\"Drainage area (m$^2$) of pixels\")\n",
    "plt.ylabel(\"Cumulative area distribution\", y=1)\n",
    "plt.savefig(\"./figure_outputs/main_curvatures.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At what drainage area do you get zero curvature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.interp(0, np.nanmedian(c_list_c, axis=0), np.logspace(2,7,70)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.interp(0, np.nanmedian(noperm_list_c, axis=0), np.logspace(2,7,70)[:-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At what drainage area do you get \"high\" curvature (1e-4)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.interp(1e-4, np.nanmedian(c_list_c, axis=0), np.logspace(2,7,70)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.interp(1e-4, np.nanmedian(noperm_list_c, axis=0), np.logspace(2,7,70)[:-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percent of the landscape is composed of negative curvature values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmedian(c_list, axis=0)[np.argmin(abs(np.nanmedian(c_list_c, axis=0)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmedian(noperm_list, axis=0)[np.argmin(abs(np.nanmedian(noperm_list_c, axis=0)))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noperm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplement_warm_shed = '7100377840'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpermafrost.loc[nonpermafrost['HYBAS_ID'] == supplement_warm_shed].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpermafrost.loc[nonpermafrost['HYBAS_ID'] == supplement_warm_shed].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplement_cold_shed = '3100413670'\n",
    "# permafrost.loc[(permafrost['bin_map'] == 2) & (permafrost['bin_relief'] == 2)].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permafrost.loc[permafrost['HYBAS_ID'] == supplement_cold_shed]['D_CN_DD']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdtopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
